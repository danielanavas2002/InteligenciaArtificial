{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Laboratorio 3**\n",
    "**Daniela Navas**\n",
    "\n",
    "## **Task 1** - Preguntas Teóricas\n",
    "\n",
    "**Responda a cada de las siguientes preguntas de forma clara y lo más completamente posible.**<br>\n",
    "**1. Explique la diferencia entre descenso de gradiente, descenso de gradiente por mini batches y descenso de gradiente estocástico. Asegúrese de mencionar las ventajas y desventajas de cada enfoque.**<br>\n",
    "El descenso de gradiente es un algoritmo fundamental en el aprendizaje automático utilizado para minimizar la función de costo en modelos. Existen tres variantes principales:<br>\n",
    "- **Descenso de Gradiente:** Este algoritmo utiliza todo el conjunto de datos para calcular el gradiente en cada iteración. Actualiza los parámetros del modelo después de procesar todos los ejemplos del conjunto de entrenamiento. Tiene como **ventajas** que proporciona una estimación precisa del gradiente y converge a un mínimo estable. Y como **desventajas** que puede ser lento para grandes conjuntos de datos y requiere mucha memoria.\n",
    "- **Descenso de Gradiente Estocástico:** Utiliza un ejemplo aleatorio del conjunto de datos en cada iteración para actualizar los parámetros. Tiene como **ventajas** ser rápido y eficiente computacionalmente y que ayuda a escapar de mínimos locales debido a su naturaleza ruidosa. Y como **desventajas** que las actualizaciones pueden ser ruidosas, lo que puede llevar a una convergencia errática.\n",
    "- **Descenso de Gradiente por Mini Batches:** Divide el conjunto de datos en lotes pequeños (mini batches) y actualiza los parámetros después del cálculo del gradiente sobre cada mini lote. Tiene como **ventajas** se equilibra la estabilidad con la velocidad y reduce la variabilidad comparado con SGD mientras mantiene cierta eficiencia computacional. Y como **desventajas** que requiere ajuste fino del tamaño del mini lote.\n",
    "\n",
    "\n",
    "**2. Compare y contraste técnicas de extracción de features (feature extraction)  y selección de features (feature selection) en machine learning. De ejemplos de escenarios donde cada técnica sería más apropiada.**<br>\n",
    "**_Extracción de Features (Feature Extraction)_**<br>\n",
    "Transforma los datos originales en un nuevo conjunto de features, generalmente de menor dimensión. Algunos **ejemplos** son PCA (Análisis de Componentes Principales), t-SNE, autoencoders. Tiene como **ventajas** que puede reducir la dimensionalidad y eliminar redundancias, mejora la interpretabilidad y la eficiencia del modelo. Y como **desventajas** que puede ser complejo y requerir un conocimiento profundo del dominio.\n",
    "La transformación puede perder información relevante. Sus **escenarios apropiados** son cuando se trabaja con datos de alta dimensionalidad y en problemas donde la reducción de dimensionalidad es crucial para la eficiencia.\n",
    "\n",
    "**_Selección de Features (Feature Selection)_**<br>\n",
    "Selecciona un subconjunto de features relevantes del conjunto de datos original sin transformarlos. Algunos **ejemplos** son selección basada en la importancia de features, selección basada en correlación, métodos de selección secuencial. Tiene como **ventajas** que mantiene la interpretabilidad de los datos originales y puede mejorar el rendimiento del modelo al eliminar features irrelevantes o redundantes. Y como **desventajas** que puede no ser tan efectiva en la reducción de dimensionalidad como la extracción de features y requiere criterios claros para la selección. Sus **escenarios apropiados** son cuando se desea mantener la interpretabilidad de los datos originales y en problemas donde algunas features pueden ser irrelevantes o redundantes.\n",
    "\n",
    "**3. Describa la arquitectura y el funcionamiento de un perceptrón de una sola capa (un tipo de red neuronal sin backpropagation). Explique cómo aprende y la forma en la que actualiza sus parámetros.**<br>\n",
    "Un perceptrón simple es una red neuronal artificial básica compuesta por una capa única que aprende mediante reglas simples sin utilizar backpropagation tradicionalmente asociada con redes neuronales profundas modernas.\n",
    "\n",
    "Su **_arquitectura_** se basa en:\n",
    "- **Entradas:** Son valores numéricos representando información inicial que alimentan al modelo.\n",
    "- **Pesos:** Cada entrada tiene un peso asociado que refleja su importancia relativa durante la predicción final.\n",
    "- **Umbral/Bias:** Un valor adicional utilizado junto con las entradas ponderadas antes aplicarse activaciones lineales o no lineales simples como paso umbral binario (\"Heaviside\").\n",
    "- **Salida:** El resultado final basado en si superó cierto umbral determinado por combinaciones ponderadas más bias ajustados durante entrenamientos iterativos usando reglas heurísticas sencillas basándose diferencias observables entre predicciones esperadas versus reales obtenidas tras probar distintos pesos hasta converger hacia soluciones óptimas dentro limitantes impuestos tanto matemáticos cómo empíricamente validables según contexto específico problema abordado.\n",
    "\n",
    "El **aprendizaje** ocurre mediante ajustes graduales basados directamente observando desviaciones sistemáticas entre salidas esperadas versus reales generando así retroalimentación positiva/negativa según sea necesario hasta lograr equilibrio deseable conforme criterios evaluativos definidos previamente. Los pesos se ajustan para minimizar el error entre la salida deseada y la salida predicha y por medio de iteración se repite para múltiples veces hasta que el error se minimice o se alcance un número máximo de iteraciones."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
