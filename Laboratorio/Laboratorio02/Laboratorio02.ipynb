{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Laboratorio 2**\n",
    "**Daniela Navas**\n",
    "\n",
    "## **Task 1** - Preguntas Teóricas\n",
    "\n",
    "**1. ¿Por qué el modelo de Naive Bayes se le considera “naive”?**<br>\n",
    "Se considera *ingenuo* debido a la suposición de que las características o predictores en un conjunto de datos son independientes entre sí, lo que se conoce como independencia condicional de clase. Esta suposición simplifica el cálculo de las probabilidades, ya que solo se necesita calcular la probabilidad de cada característica de forma individual, sin considerar las interacciones entre ellas. Aunque esta suposición no siempre se cumple en la realidad, donde las características suelen estar interrelacionadas, el algoritmo sigue siendo efectivo y fácil de implementar, especialmente con grandes conjuntos de datos.\n",
    "\n",
    "\n",
    "**2. Explique la formulación matemática que se busca optimizar en Support Vector Machine, además responda ¿cómo funciona el truco del Kernel para este modelo? (Lo que se espera de esta pregunta es que puedan explicar en sus propias palabras la fórmula a la que llegamos que debemos optimizar de SVM en clase)**<br>\n",
    "**Support Vector Machine**<br>\n",
    "En Support Vector Machine (SVM), el objetivo es encontrar el hiperplano que mejor separa las clases en el espacio de características.<br> \n",
    "Primero se determina un hiperplano que separe dos clases de datos en un espacio de características. El hiperplano se define por la ecuación:\n",
    "\n",
    "$$\\mathbf{w} \\cdot \\mathbf{x} + b = 0$$\n",
    "\n",
    "donde:\n",
    "- Vector de pesos: $ \\mathbf{w} $\n",
    "- Sesgo (bias): $ b $\n",
    "- Vector de características: $ \\mathbf{x} $\n",
    "\n",
    "\n",
    "Para que los datos estén correctamente clasificados, se deben cumplir las siguientes condiciones:\n",
    "- Para puntos de la clase $+1$: $ \\mathbf{w} \\cdot \\mathbf{x}_i + b \\geq 1 $\n",
    "- Para puntos de la clase $-1$: $ \\mathbf{w} \\cdot \\mathbf{x}_i + b \\leq -1 $\n",
    "\n",
    "Combinandolas en una sola:\n",
    "\n",
    "$$y_i (\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1 \\quad \\forall i$$\n",
    "\n",
    "donde $y_i$ es la etiqueta de la clase (+1 o -1).\n",
    "\n",
    "El margen es la distancia entre los puntos más cercanos de cada clase y el hiperplano. Queremos maximizar este margen. La distancia de un punto al hiperplano es:\n",
    "\n",
    "$$\\frac{1}{\\|\\mathbf{w}\\|}$$\n",
    "\n",
    "Para maximizar el margen, minimizamos $\\|\\mathbf{w}\\|$. Sin embargo, para simplificar los cálculos, minimizamos $\\frac{1}{2} \\|\\mathbf{w}\\|^2$ en su lugar.\n",
    "\n",
    "El problema de optimización se formula como:\n",
    "\n",
    "$$\\min_{\\mathbf{w}, b} \\frac{1}{2} \\|\\mathbf{w}\\|^2$$\n",
    "\n",
    "sujeto a las restricciones:\n",
    "\n",
    "$$y_i (\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\geq 1 \\quad \\forall i$$\n",
    "\n",
    "Esto nos lleva a un problema de optimización cuadrática con restricciones lineales. Se puede resolver usando métodos como los multiplicadores de Lagrange y técnicas de programación cuadrática.\n",
    "\n",
    "Si los datos no son linealmente separables en el espacio original, utilizamos el truco del Kernel para mapear los datos a un espacio de mayor dimensión donde sí sean separables. Esto se hace mediante una función de kernel $K(\\mathbf{x}_i, \\mathbf{x}_j)$ que calcula el producto interno en el espacio transformado.\n",
    "\n",
    "**Truco del Kernel**<br>\n",
    "El truco del Kernel permite a SVM manejar datos que no son linealmente separables en el espacio original de características. La idea es transformar los datos a un espacio de mayor dimensión donde sí sean linealmente separables. Esto se hace mediante una función de kernel $K(\\mathbf{x}_i, \\mathbf{x}_j)$ que calcula el producto interno en el espacio transformado sin necesidad de calcular explícitamente la transformación.\n",
    "\n",
    "Si hay un conjunto de datos $\\{\\mathbf{x}_i, y_i\\}$ donde $\\mathbf{x}_i$ es el vector de características y $y_i$ es la etiqueta de clase. En el espacio original, los datos no son linealmente separables.\n",
    "\n",
    "Queremos transformar los datos a un espacio de mayor dimensión utilizando una función de transformación $\\phi(\\mathbf{x})$. Sin embargo, calcular explícitamente $\\phi(\\mathbf{x})$ puede ser computacionalmente costoso.\n",
    "\n",
    "En lugar de calcular $\\phi(\\mathbf{x})$ directamente, utilizamos una función de kernel $K(\\mathbf{x}_i, \\mathbf{x}_j)$ que calcula el producto interno en el espacio transformado:\n",
    "\n",
    "$$K(\\mathbf{x}_i, \\mathbf{x}_j) = \\phi(\\mathbf{x}_i) \\cdot \\phi(\\mathbf{x}_j)$$\n",
    "\n",
    "\n",
    "**3. Investigue sobre Random Forest y responda**<br>  \n",
    "**a. ¿Qué tipo de ensemble learning es este modelo?**<br> \n",
    "Random Forest es un ejemplo de bagging (bootstrap aggregating). En este enfoque, se generan múltiples modelos (en este caso, árboles de decisión) entrenados en diferentes subconjuntos de datos obtenidos mediante muestreo con reemplazo. Este método busca reducir la varianza y mejorar la estabilidad del modelo al promediar las predicciones de varios árboles.\n",
    "\n",
    "**b. ¿Cuál es la idea general detrás de Random Forest?**<br> \n",
    "Crear un conjunto de árboles de decisión que trabajan juntos para mejorar la precisión de las predicciones. Cada árbol es entrenado con una muestra aleatoria de los datos y, durante el proceso de construcción, se selecciona un subconjunto aleatorio de características para cada división en el árbol. Esto permite que los árboles sean diversos y, al final, sus resultados se combinan mediante votación (en clasificación) o promediación (en regresión) para obtener una predicción final más robusta.\n",
    "\n",
    "**c. ¿Por qué se busca baja correlación entre los árboles de Random Forest?**<br>\n",
    "La baja correlación entre los árboles en un modelo de Random Forest es crucial porque permite que cada árbol capture diferentes patrones en los datos. Si los árboles están muy correlacionados, sus errores también lo estarán, lo que puede llevar a una reducción limitada en la varianza del modelo. Al promover la diversidad entre los árboles, se mejora la capacidad del modelo para generalizar a nuevos datos y se reduce el riesgo de sobreajuste. Este enfoque asegura que la combinación de las predicciones sea más efectiva y robusta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task 2** -  Naive Bayes: Clasificador de Mensajes Ham/Spam<br>\n",
    "Deberá construir un programa que reciba como entrada un archivo llamado “entrenamiento.txt” que será su dataset para entrenar un modelo basado en Bayes/Laplace Smoothing para clasificar mensajes como ham o spam. De dicho modelo, deberá reportar alguna métrica de desempeño. Además, con dicho modelo deberá ser capaz de interpretar mensajes futuros como spam o ham. Asimismo, deberá considerar las siguientes restricciones: \n",
    "- Solamente se podrá entrenar un modelo por dataset de entrenamiento (No se pueden cargar más de un archivo para entrenar el modelo) \n",
    "- Deberá limpiar el dataset de caracteres especiales y de combinaciones de mayúsculas/minúsculas. \n",
    "- Cada línea representa una observación (mensaje con su respectiva categoría) \n",
    "-  Del dataset dado deberá dividirlo en training y test. \n",
    "    - Para este punto, podrá usar librerías externas como las dadas en sklearn.  \n",
    "    - **NO** se aceptará el uso de librerías para la construcción del modelo principal\n",
    "\n",
    "### **Task 2.1** - Lectura y limpieza del dataset<br> \n",
    "Reciba como entrada un archivo llamado “entrenamiento.txt” que tendrá una estructura como la que se muestra \n",
    "abajo. En dicho archivo, cada línea representa un mensaje, y la primera palabra de la línea indica si es un mensaje \n",
    "Ham o Spam (etiqueta/categoría del mensaje), luego encuentra una tabulación (\\t) para separar la etiqueta del \n",
    "verdadero mensaje, finalmente está el mensaje que termina hasta que encuentre un cambio de línea (\\n). En base a \n",
    "este archivo, usted deberá entrenar un modelo basado en Bayes con Laplace Smoothing para clasificar mensajes \n",
    "como spam o ham.<br>  \n",
    "Además, deberá limpiar el archivo de texto, teniendo en cuenta que pueden haber caracteres especiales que \n",
    "solamente agreguen ruido al mensaje y otros que pueden llegar a ser de utilidad para la clasificación. De igual modo, \n",
    "se recomienda cambiar los mensajes a que todos sigan la misma nomenclatura, es decir, todo en mayúsculas o bien, \n",
    "todo en minúscula, o similar, de modo tal que pueda clasificar de mejor manera la participación de cada palabra para \n",
    "determinar la categoría del mensaje.<br>   \n",
    "También, deberá separar el dataset con un 80% para training, 20% para testing. Si llegan a necesitar una parte para \n",
    "validation, pueden subdividir el 20% de testing en 10% para validation y 10% para testing. Recuerden que esta división \n",
    "siempre deberá ser realizada de forma aleatoria, para reproducir sus resultados usen una seed.<br>  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Abrir archivo \n",
    "with open(\"entrenamiento.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Convertir en un set donde se tenga la etiqueta y el mensaje\n",
    "data = []\n",
    "for line in lines:\n",
    "    parts = line.strip().split(\"\\t\", 1)  # Separar solo en la primera tabulación\n",
    "    if len(parts) == 2:\n",
    "        label, message = parts\n",
    "        data.append((label, message))\n",
    "\n",
    "# Convertir a DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"label\", \"message\"])\n",
    "\n",
    "# Limpieza de datos | Eliminar ruido\n",
    "def clean_text(text):\n",
    "    text = text.lower()                      # Convertir a minúsculas\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)  # Eliminar caracteres especiales\n",
    "    return text\n",
    "\n",
    "df[\"message\"] = df[\"message\"].apply(clean_text)\n",
    "\n",
    "# División en conjuntos de entrenamiento (80%) y prueba (20%)\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 2.2** - Construcción del modelo<br> \n",
    "Una vez tenga una forma de leer y limpiar el dataset, deberá crear un modelo basado en Bayes con Laplace Smoothing para que clasifique los mensajes en spam o ham. Esto deberá basarse en la probabilidad de cada palabra en pertenecer a cada uno de los posibles grupos. Cuando tenga la probabilidad del mensaje que sea spam o ham, deberá clasificarlo con la probabilidad de la categoría que resulte mayor.<br>  \n",
    "Es importante tomar en cuenta que el entrenamiento y construcción del modelo deberá hacerse usando solamente la parte de training del dataset.<br>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión en el conjunto de prueba: 0.9641\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.metrics import accuracy_score\n",
    " \n",
    "def entrenar_bayes(mensajes, etiquetas, alpha=1):\n",
    "    vocabulario = set()\n",
    "    conteo_palabras = {'ham': defaultdict(int), 'spam': defaultdict(int)}\n",
    "    total_palabras = {'ham': 0, 'spam': 0}\n",
    "    total_mensajes = {'ham': 0, 'spam': 0}\n",
    "    \n",
    "    for mensaje, etiqueta in zip(mensajes, etiquetas):\n",
    "        total_mensajes[etiqueta] += 1\n",
    "        palabras = mensaje.split()\n",
    "        for palabra in palabras:\n",
    "            vocabulario.add(palabra)\n",
    "            conteo_palabras[etiqueta][palabra] += 1\n",
    "            total_palabras[etiqueta] += 1\n",
    "    \n",
    "    return vocabulario, conteo_palabras, total_palabras, total_mensajes\n",
    "\n",
    "def calcular_probabilidades(mensaje, vocabulario, conteo_palabras, total_palabras, total_mensajes, alpha=1):\n",
    "    categorias = ['ham', 'spam']\n",
    "    total_mensajes_total = sum(total_mensajes.values())\n",
    "    probabilidades = {}\n",
    "    palabras = mensaje.split()\n",
    "    \n",
    "    for categoria in categorias:\n",
    "        prob_categoria = np.log(total_mensajes[categoria] / total_mensajes_total)\n",
    "        prob_palabras = 0\n",
    "        for palabra in palabras:\n",
    "            conteo = conteo_palabras[categoria][palabra] + alpha\n",
    "            prob_palabras += np.log(conteo / (total_palabras[categoria] + alpha * len(vocabulario)))\n",
    "        probabilidades[categoria] = prob_categoria + prob_palabras\n",
    "    \n",
    "    return max(probabilidades, key=probabilidades.get)\n",
    "\n",
    "def predecir(mensajes, vocabulario, conteo_palabras, total_palabras, total_mensajes):\n",
    "    return [calcular_probabilidades(mensaje, vocabulario, conteo_palabras, total_palabras, total_mensajes) for mensaje in mensajes]\n",
    "\n",
    "# Entrenar modelo\n",
    "vocabulario, conteo_palabras, total_palabras, total_mensajes = entrenar_bayes(train_data[\"message\"], train_data[\"label\"])\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "predicciones = predecir(test_data[\"message\"], vocabulario, conteo_palabras, total_palabras, total_mensajes)\n",
    "\n",
    "# Evaluar modelo\n",
    "precision = accuracy_score(test_data[\"label\"], predicciones)\n",
    "print(f\"Precisión del modelo: {precision:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando ya tengan entrenado su modelo, deberán probar el modelo usando la parte de testing del dataset. La métrica a utilizar deberá proponerla considerando la distribución de clases/categorías en el dataset. Recuerde dejar justificada su respuesta en los comentarios de su código.<br>  \n",
    "Presente al final del entrenamiento, la métrica de desempeño sobre el subset de training y sobre el subset de testing.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "from collections import defaultdict\n",
    "\n",
    "# Función para limpiar los mensajes\n",
    "def limpiar_mensaje(mensaje):\n",
    "    # Convertir a minúsculas\n",
    "    mensaje = mensaje.lower()\n",
    "    \n",
    "    # Eliminar caracteres no alfabéticos (excepto espacios)\n",
    "    mensaje = ''.join([c if c.isalpha() or c == ' ' else '' for c in mensaje])\n",
    "    \n",
    "    return mensaje\n",
    "\n",
    "# Función para leer el archivo y limpiar los datos\n",
    "def leer_datos(archivo):\n",
    "    datos = []\n",
    "    with open(archivo, 'r') as f:\n",
    "        for linea in f:\n",
    "            # Separar la etiqueta y el mensaje\n",
    "            etiqueta, mensaje = linea.split('\\t', 1)\n",
    "            mensaje = limpiar_mensaje(mensaje.strip())\n",
    "            datos.append((etiqueta, mensaje))\n",
    "    return datos\n",
    "\n",
    "# Función para dividir el dataset en training, validation y test\n",
    "def dividir_dataset(datos, seed=42):\n",
    "    random.seed(seed)\n",
    "    random.shuffle(datos)\n",
    "    \n",
    "    # Dividir en 80% entrenamiento, 10% validación y 10% prueba\n",
    "    total = len(datos)\n",
    "    train_data = datos[:int(0.8 * total)]\n",
    "    temp_data = datos[int(0.8 * total):]\n",
    "    valid_data = temp_data[:int(0.5 * len(temp_data))]\n",
    "    test_data = temp_data[int(0.5 * len(temp_data)):]\n",
    "    \n",
    "    return train_data, valid_data, test_data\n",
    "\n",
    "# Función para entrenar el modelo Naive Bayes con Laplace Smoothing\n",
    "def entrenar_naive_bayes(train_data):\n",
    "    contador_spam = defaultdict(int)\n",
    "    contador_ham = defaultdict(int)\n",
    "    total_spam = 0\n",
    "    total_ham = 0\n",
    "    vocabulario = set()\n",
    "    \n",
    "    # Contar palabras en spam y ham\n",
    "    for etiqueta, mensaje in train_data:\n",
    "        palabras = mensaje.split()\n",
    "        if etiqueta == 'spam':\n",
    "            total_spam += len(palabras)\n",
    "            for palabra in palabras:\n",
    "                contador_spam[palabra] += 1\n",
    "                vocabulario.add(palabra)\n",
    "        else:\n",
    "            total_ham += len(palabras)\n",
    "            for palabra in palabras:\n",
    "                contador_ham[palabra] += 1\n",
    "                vocabulario.add(palabra)\n",
    "    \n",
    "    V = len(vocabulario)  # Número de palabras únicas en el vocabulario\n",
    "    \n",
    "    # Calcular probabilidades\n",
    "    prob_spam = len([1 for etiqueta, _ in train_data if etiqueta == 'spam']) / len(train_data)\n",
    "    prob_ham = len([1 for etiqueta, _ in train_data if etiqueta == 'ham']) / len(train_data)\n",
    "    \n",
    "    return contador_spam, contador_ham, total_spam, total_ham, prob_spam, prob_ham, V\n",
    "\n",
    "# Función para clasificar un mensaje\n",
    "def clasificar_mensaje(mensaje, contador_spam, contador_ham, total_spam, total_ham, prob_spam, prob_ham, V):\n",
    "    palabras = mensaje.split()\n",
    "    \n",
    "    # Probabilidades a priori de spam y ham\n",
    "    probabilidad_spam = prob_spam\n",
    "    probabilidad_ham = prob_ham\n",
    "    \n",
    "    # Probabilidades de las palabras condicionadas a las clases\n",
    "    for palabra in palabras:\n",
    "        probabilidad_spam *= (contador_spam[palabra] + 1) / (total_spam + V)\n",
    "        probabilidad_ham *= (contador_ham[palabra] + 1) / (total_ham + V)\n",
    "    \n",
    "    # Clasificar el mensaje\n",
    "    if probabilidad_spam > probabilidad_ham:\n",
    "        return 'spam', probabilidad_spam, probabilidad_ham\n",
    "    else:\n",
    "        return 'ham', probabilidad_ham, probabilidad_spam\n",
    "\n",
    "# Función para evaluar el modelo\n",
    "def evaluar_modelo(test_data, contador_spam, contador_ham, total_spam, total_ham, prob_spam, prob_ham, V):\n",
    "    correctos = 0\n",
    "    total = len(test_data)\n",
    "    \n",
    "    for etiqueta, mensaje in test_data:\n",
    "        prediccion = clasificar_mensaje(mensaje, contador_spam, contador_ham, total_spam, total_ham, prob_spam, prob_ham, V)\n",
    "        if prediccion[0] == etiqueta:\n",
    "            correctos += 1\n",
    "    \n",
    "    # Métrica de desempeño (precisión)\n",
    "    precision = correctos / total\n",
    "    return precision\n",
    "\n",
    "# Leer y limpiar datos\n",
    "datos = leer_datos('entrenamiento.txt')\n",
    "\n",
    "# Dividir los datos\n",
    "train_data, valid_data, test_data = dividir_dataset(datos)\n",
    "\n",
    "# Entrenar el modelo\n",
    "contador_spam, contador_ham, total_spam, total_ham, prob_spam, prob_ham, V = entrenar_naive_bayes(train_data)\n",
    "\n",
    "# Evaluar el modelo en el conjunto de prueba\n",
    "precision_test = evaluar_modelo(test_data, contador_spam, contador_ham, total_spam, total_ham, prob_spam, prob_ham, V)\n",
    "\n",
    "print(f'Precisión en el conjunto de prueba: {precision_test:.4f}')\n",
    "\n",
    "# Interfaz interactiva para clasificar nuevos mensajes\n",
    "def clasificar_nuevo_mensaje():\n",
    "    while True:\n",
    "        mensaje = input(\"Ingrese un mensaje para clasificar (o 'salir' para terminar): \")\n",
    "        \n",
    "        if mensaje.lower() == 'salir':\n",
    "            print(\"Terminando la clasificación.\")\n",
    "            break\n",
    "        \n",
    "        mensaje_limpio = limpiar_mensaje(mensaje)\n",
    "        \n",
    "        # Clasificar el mensaje\n",
    "        clasificacion, probabilidad_spam, probabilidad_ham = clasificar_mensaje(mensaje_limpio, \n",
    "                                                                                 contador_spam, \n",
    "                                                                                 contador_ham, \n",
    "                                                                                 total_spam, \n",
    "                                                                                 total_ham, \n",
    "                                                                                 prob_spam, \n",
    "                                                                                 prob_ham, \n",
    "                                                                                 V)\n",
    "        \n",
    "        print(f\"Clasificación: {clasificacion}\")\n",
    "        print(f\"Probabilidad de Spam: {probabilidad_spam:.4f}\")\n",
    "        print(f\"Probabilidad de Ham: {probabilidad_ham:.4f}\")\n",
    "        \n",
    "# Llamar la función para la interfaz interactiva\n",
    "clasificar_nuevo_mensaje()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
