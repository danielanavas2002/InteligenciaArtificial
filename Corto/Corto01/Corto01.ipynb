{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Corto 1** \n",
    "\n",
    "**Daniela Navas**\n",
    "\n",
    "---\n",
    "\n",
    "## **Task** - Frozen Lake\n",
    "\n",
    "El juego de Frozen Lake consiste en llevar al personaje de un punto de inicio hasta la meta, cuidando no caer en ningún agujero, además se debe considerar que algunas veces el jugador no se moverá en la dirección esperada debido a la naturaleza resbaladiza del hielo.  \n",
    "\n",
    "Este juego se desempeña en un tablero de 4x4, estando siempre la meta en el último cuadro. Los agujeros están distribuidos de forma aleatoria, además que cada vez que se genere el juego debe de generarse un nuevo mundo.\n",
    "\n",
    "Para este task, no debe implementar todo desde cero, por el contrario debe centrarse en la definición de la política o método de aprendizaje a ser utilizado (Q-learning, MDP libre de modelos, MDP basado en modelos, etc). Por ello, usará la librería de Gymnasium (previamente soportada por OpenIA) para poder desarrollar una solución para este juego.\n",
    "\n",
    "Usando la mencionada librería tendrá implementandos ya el espacio de acciones, el espacio observable, el estado inicial, el conjunto de acciones, los finales de episodio, además de funciones que le darán información. Para mayor información visiten este [enlace](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)\n",
    ".  \n",
    "\n",
    "En conclusión, su trabajo para este task es implementar un agente inteligente que resuelva el juego de Frozen Lake con el argumento de slippery=True, usando un algoritmo de aprendizaje por refuerzo.  \n",
    "\n",
    "---\n",
    "\n",
    "**Q-learning**\n",
    "\n",
    "Se usará Q-learning como método de aprendizaje, ya que maneja entornos estocásticos, como el que presenta la opción de *slippery=True* con una incertidumbre en los movimientos debido al hielo resbaladizo. Aprende la política óptima a través de la exploración y explotación, sin necesidad de un modelo del entorno. Además, es sencillo de implementar, escalable y se adapta perfectamente a los entornos discretos y dinámicos proporcionados por Gymnasium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerias\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import io\n",
    "from IPython.display import display, Image\n",
    "import random\n",
    "\n",
    "# Q-learning\n",
    "alpha = 0.1           # Tasa de aprendizaje\n",
    "gamma = 0.99          # Factor de descuento\n",
    "epsilon = 1.0         # Tasa de exploración\n",
    "epsilon_decay = 0.995    \n",
    "epsilon_min = 0.01         \n",
    "episodes = 2000       # Episodios\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "#\n",
    "# Función para generar mapa\n",
    "#\n",
    "# ------------------------------------------------------------------\n",
    "def generate_random_map(size=4, p_hole=0.25):      # Mapa aleatorio con max 4 agujeros\n",
    "    map_ = np.full((size, size), \"F\", dtype=\"U1\")  # Crear un mapa lleno de 'F' (Frozen)\n",
    "    map_[0, 0] = \"S\"                               # Colocar la posición de inicio en la esquina superior izquierda\n",
    "    map_[-1, -1] = \"G\"                             # Colocar la meta en la esquina inferior derecha\n",
    "    \n",
    "    # Colocar hasta 4 agujeros aleatorios ('H')\n",
    "    holes = random.sample(range(1, size * size - 1), k=min(4, size * size - 2))\n",
    "    for hole in holes:\n",
    "        i, j = divmod(hole, size)\n",
    "        map_[i, j] = \"H\"  # Colocar agujeros aleatorios en el mapa\n",
    "    \n",
    "    return [\"\".join(row) for row in map_]  # Convertir mapa en una lista de strings\n",
    "\n",
    "\n",
    "# Crear el entorno FrozenLake con un mapa aleatorio, con argumento is_slippery como True\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True, desc=generate_random_map(), render_mode=None)\n",
    "n_states = env.observation_space.n    # Número de estados\n",
    "n_actions = env.action_space.n        # Número de acciones posibles\n",
    "\n",
    "Q_table = np.zeros((n_states, n_actions)) # Inicializar tabla Q (de dimensiones [estados x acciones] con valores 0)\n",
    "rewards = [] # Lista para almacenar las recompensas obtenidas durante el entrenamiento\n",
    "\n",
    "# /////////////////////////////////////////////////\n",
    "#                                                //\n",
    "# Entrenamiento de Modelo                        //\n",
    "#                                                //\n",
    "# /////////////////////////////////////////////////\n",
    "for episode in range(episodes):\n",
    "    state, _ = env.reset()  # Inicializar el entorno y obtener el estado inicial\n",
    "    total_reward = 0        # Recompensa total acumulada durante el episodio\n",
    "    done = False            # Bandera si el episodio yaa termino\n",
    "    \n",
    "    while not done:  # Ejecutar el episodio hasta que termine\n",
    "        # Def si explorar o explotar usando epsilon-greedy\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample()  # Exploración (selección aleatoria de acción)\n",
    "        else:\n",
    "            action = np.argmax(Q_table[state, :])  # Explotación (selección de la mejor acción conocida)\n",
    "        \n",
    "        # Realizar la acción y obtener el siguiente estado, recompensa y si ha terminado el episodio\n",
    "        next_state, reward, done, _, _ = env.step(action)  \n",
    "        \n",
    "        # Actualizar la tabla Q usando la ecuación de Q-learning\n",
    "        Q_table[state, action] = Q_table[state, action] + alpha * (\n",
    "            reward + gamma * np.max(Q_table[next_state, :]) - Q_table[state, action]\n",
    "        )\n",
    "        \n",
    "        # Actualizar el estado actual\n",
    "        state = next_state\n",
    "        total_reward += reward # Sumar la recompensa obtenida\n",
    "    \n",
    "    rewards.append(total_reward)                         # Guardar la recompensa del episodio\n",
    "    epsilon = max(epsilon * epsilon_decay, epsilon_min)  # Reducir epsilon para hacer menos exploración\n",
    "    \n",
    "# /////////////////////////////////////////////////\n",
    "#                                                //\n",
    "# Evaluación del agente                          //\n",
    "#                                                //\n",
    "# /////////////////////////////////////////////////\n",
    "successes = 0        # Contador de episodios exitosos (donde llega a la meta)\n",
    "test_episodes = 100  # Número de episodios para probar el agente\n",
    "optimal_path = []    # Lista para almacenar el mejor camino encontrado\n",
    "\n",
    "# Evaluación en episodios de prueba\n",
    "for _ in range(test_episodes):\n",
    "    state, _ = env.reset()     # Inicializar el entorno para cada prueba\n",
    "    done = False\n",
    "    episode_path = []          # Lista para almacenar el camino seguido en el episodio\n",
    "    while not done:\n",
    "        episode_path.append(state)                    # Guardar el estado en el camino\n",
    "        action = np.argmax(Q_table[state, :])         # Tomar la acción con la mayor Q\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "    if reward == 1:                  # Si el agente llega a la meta\n",
    "        successes += 1               # Contar el éxito\n",
    "        optimal_path = episode_path  # Guardar el camino exitoso\n",
    "\n",
    "# MOSTRAR TASA DE ÉXITO\n",
    "print(f\"Tasa de éxito en {test_episodes} episodios: {successes / test_episodes * 100:.2f}%\")\n",
    "\n",
    "# /////////////////////////////////////////////////\n",
    "#                                                //\n",
    "# GRÁFICA DE CONVERGENCIA                        //\n",
    "#                                                //\n",
    "# /////////////////////////////////////////////////\n",
    "plt.plot(np.convolve(rewards, np.ones(100)/100, mode='valid'))\n",
    "plt.xlabel(\"Episodios\")\n",
    "plt.ylabel(\"Recompensa\")\n",
    "plt.title(\"Convergencia de Q-learning | Frozen Lake\")\n",
    "plt.show()\n",
    "\n",
    "# *************************************************************************************************\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "#\n",
    "# Función para hacer GIF | Mostrar gráficamente el camino\n",
    "#\n",
    "# ------------------------------------------------------------------\n",
    "def create_gif(optimal_path):\n",
    "    size = int(np.sqrt(n_states))  # Tamaño del mapa (4x4)\n",
    "    lake = np.array([\"F\"] * n_states).reshape((size, size))  # Crear el mapa base\n",
    "    lake[0, 0] = \"S\"                                         # Colocar la casilla de inicio\n",
    "    lake[-1, -1] = \"G\"                                       # Colocar la casilla de meta\n",
    "    \n",
    "    env_unwrapped = env.unwrapped  # Acceder al entorno subyacente para obtener los agujeros\n",
    "    for state in range(n_states):\n",
    "        if env_unwrapped.desc.flatten()[state] == b\"H\":\n",
    "            i, j = divmod(state, size)\n",
    "            lake[i, j] = \"H\"  # Colocar agujeros en el mapa\n",
    "    \n",
    "    # Colores para las casillas (hielo, agujerso, inicio, meta)\n",
    "    colors = {\"F\": \"skyblue\", \"H\": \"black\", \"S\": \"magenta\", \"G\": \"green\"}\n",
    "    frames = []   # Lista para almacenar los frames del GIF\n",
    "    \n",
    "    # Generar los frames del GIF recorriendo el camino del agente\n",
    "    for step, state in enumerate(optimal_path):\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        # Dibujar todos los cuadros del mapa\n",
    "        for i in range(size):\n",
    "            for j in range(size):\n",
    "                ax.add_patch(plt.Rectangle((j, size-i-1), 1, 1, color=colors[lake[i, j]]))\n",
    "        \n",
    "        # Dibujar el camino recorrido por el agente en azul\n",
    "        for s in optimal_path[:step+1]:\n",
    "            i, j = divmod(s, size)\n",
    "            ax.add_patch(plt.Rectangle((j, size-i-1), 1, 1, color=\"blue\", alpha=0.6))\n",
    "\n",
    "        ax.set_xticks([])     # Quitar marcas en el eje x y y\n",
    "        ax.set_yticks([])     \n",
    "        ax.set_xlim(0, size)  # Establecer límites del gráfico\n",
    "        ax.set_ylim(0, size)  \n",
    "        plt.title(\"Frozen Lake\")  # Título\n",
    "\n",
    "        # Guardar el frame\n",
    "        fig.canvas.draw()\n",
    "        image = np.array(fig.canvas.renderer.buffer_rgba())  # Convertir el gráfico a imagen\n",
    "        frames.append(image)                                 # Añadir el frame a la lista de frames\n",
    "        plt.close(fig)                                       # Cerrar la figura para no sobrecargar la memoria\n",
    "\n",
    "    # Generar un frame final | Juego Ganado\n",
    "    fig, ax = plt.subplots()\n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            ax.add_patch(plt.Rectangle((j, size-i-1), 1, 1, color=colors[lake[i, j]]))\n",
    "\n",
    "    for s in optimal_path:\n",
    "        i, j = divmod(s, size)\n",
    "        ax.add_patch(plt.Rectangle((j, size-i-1), 1, 1, color=\"blue\", alpha=0.6))\n",
    "\n",
    "    i, j = divmod(n_states - 1, size)  \n",
    "    ax.add_patch(plt.Rectangle((j, size-i-1), 1, 1, color=\"blue\", alpha=0.6))\n",
    "\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlim(0, size)\n",
    "    ax.set_ylim(0, size)\n",
    "    plt.title(\"Frozen Lake\")\n",
    "\n",
    "    # Guardar el frame final y agregarlo a los frames\n",
    "    fig.canvas.draw()\n",
    "    image = np.array(fig.canvas.renderer.buffer_rgba())\n",
    "    frames.append(image)\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # Mostrar el GIF final con todos los frames generados\n",
    "    buffer = io.BytesIO()\n",
    "    imageio.mimsave(buffer, frames, duration=0.5, format='GIF')  # Guardar el GIF\n",
    "    display(Image(data=buffer.getvalue()))                       # Mostrar el GIF en la interfaz\n",
    "\n",
    "# Crear y mostrar el GIF con la solución\n",
    "if optimal_path:\n",
    "    create_gif(optimal_path) # Llamar a la función para crear y mostrar el GIF con el camino del agente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**GITHUB:**\n",
    "https://github.com/danielanavas2002/InteligenciaArtificial/tree/main/Corto/Corto01\n",
    "\n",
    "**VIDEO DE FUNCIONAMIENTO:** https://youtu.be/7b1zcZVFy9E\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
